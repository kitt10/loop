{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "COCO_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(COCO_NAMES), 3))\n",
    "\n",
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def get_model(device):\n",
    "    model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(\n",
    "        #weights='DEFAULT',\n",
    "        weights_backbone='DEFAULT', \n",
    "        trainable_backbone_layers=0,\n",
    "        num_classes=2\n",
    "    )\n",
    "    model = model.eval().to(device)\n",
    "    return model\n",
    "\n",
    "def predict(image, model, device, detection_threshold):\n",
    "    \"\"\"\n",
    "    Predict the output of an image after forward pass through\n",
    "    the model and return the bounding boxes, class names, and \n",
    "    class labels. \n",
    "    \"\"\"\n",
    "    # transform the image to tensor\n",
    "    image = TRANSFORM(image).to(device)\n",
    "    # add a batch dimension\n",
    "    image = image.unsqueeze(0) \n",
    "    # get the predictions on the image\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image) \n",
    "    # get score for all the predicted objects\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    # get all the predicted bounding boxes\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "    # get boxes above the threshold score\n",
    "    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "\n",
    "    labels = outputs[0]['labels'][:len(boxes)]\n",
    "    # get all the predicited class names\n",
    "    pred_classes = [COCO_NAMES[i] for i in labels.cpu().numpy()]\n",
    "    return boxes, pred_classes, labels\n",
    "\n",
    "def draw_boxes(boxes, classes, labels, image):\n",
    "    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "    for i, box in enumerate(boxes):\n",
    "        color = COLORS[labels[i]]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 2\n",
    "        )\n",
    "        cv2.putText(image, classes[i], (int(box[0]), int(box[1]-5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_model(device)\n",
    "print([p.size() for p in model.parameters()])\n",
    "image = Image.open('i2l-dataset/ball/ball_01.jpg')\n",
    "\n",
    "boxes, classes, labels = predict(image, model, device, detection_threshold=0.1)\n",
    "print([COCO_NAMES[label] for label in labels])\n",
    "image = draw_boxes(boxes, classes, labels, image)\n",
    "\n",
    "save_name = f'ssd_out-test'\n",
    "#cv2.imshow('Image', image)\n",
    "cv2.imwrite(f'{save_name}.jpg', image)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kitt/miniconda3/envs/trans/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "# YOLOS\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from PIL import Image\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "#url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "#image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
    "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "M = 3\n",
    "last_layer = nn.Linear(in_features=model.class_labels_classifier.layers[2].in_features, out_features=M)\n",
    "model.class_labels_classifier.layers[2] = last_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label: ball\n",
      "Detected mug with confidence 0.734 at location [28.31, 13.28, 168.5, 87.82]\n",
      "Detected mug with confidence 0.706 at location [10.33, 12.57, 173.29, 221.33]\n",
      "Detected mug with confidence 0.677 at location [11.34, 16.24, 128.02, 200.25]\n",
      "Detected mug with confidence 0.661 at location [12.07, 162.79, 73.07, 211.01]\n",
      "Detected mug with confidence 0.678 at location [72.37, 90.88, 173.91, 210.64]\n",
      "Detected mug with confidence 0.715 at location [14.49, 13.76, 216.98, 204.35]\n",
      "Detected mug with confidence 0.693 at location [0.06, 10.12, 224.84, 174.39]\n",
      "Detected mug with confidence 0.647 at location [129.42, 68.01, 209.8, 193.32]\n",
      "Detected mug with confidence 0.547 at location [15.61, 155.53, 213.48, 221.07]\n",
      "Detected mug with confidence 0.675 at location [33.88, 17.47, 165.23, 150.71]\n",
      "true label: ball\n",
      "Detected mug with confidence 0.703 at location [86.6, 89.59, 329.41, 269.46]\n",
      "Detected mug with confidence 0.66 at location [8.34, 119.12, 250.41, 582.67]\n",
      "Detected mug with confidence 0.545 at location [19.31, 112.23, 286.98, 468.57]\n",
      "Detected mug with confidence 0.546 at location [35.8, 404.36, 460.78, 596.79]\n",
      "Detected mug with confidence 0.703 at location [149.68, 264.92, 394.47, 502.99]\n",
      "Detected mug with confidence 0.656 at location [40.0, 87.52, 466.76, 571.67]\n",
      "Detected mug with confidence 0.521 at location [21.7, 88.89, 510.36, 446.43]\n",
      "Detected mug with confidence 0.592 at location [228.62, 166.68, 481.73, 577.33]\n",
      "Detected mug with confidence 0.692 at location [133.78, 168.71, 284.3, 323.62]\n",
      "Detected mug with confidence 0.606 at location [98.27, 165.91, 289.59, 384.97]\n",
      "true label: ball\n",
      "Detected mug with confidence 0.783 at location [212.52, 0.0, 259.34, 1.96]\n",
      "Detected mug with confidence 0.785 at location [68.54, 17.41, 184.41, 173.85]\n",
      "Detected mug with confidence 0.562 at location [53.69, 56.3, 81.09, 114.89]\n",
      "Detected mug with confidence 0.569 at location [74.34, 43.83, 171.74, 139.51]\n",
      "Detected mug with confidence 0.71 at location [52.54, 18.26, 208.28, 164.79]\n",
      "Detected mug with confidence 0.713 at location [51.53, 18.29, 213.99, 158.82]\n",
      "Detected mug with confidence 0.505 at location [73.76, 19.02, 218.86, 167.23]\n",
      "Detected mug with confidence 0.594 at location [55.8, 92.67, 206.42, 175.58]\n",
      "Detected mug with confidence 0.688 at location [80.66, 48.82, 144.42, 104.49]\n",
      "Detected mug with confidence 0.615 at location [65.5, 46.78, 149.33, 125.17]\n",
      "true label: ball\n",
      "Detected mug with confidence 0.79 at location [0.5, 0.53, 51.68, 45.53]\n",
      "Detected mug with confidence 0.781 at location [0.07, 27.79, 10.83, 598.73]\n",
      "Detected mug with confidence 0.762 at location [106.07, 115.5, 329.14, 475.26]\n",
      "Detected mug with confidence 0.772 at location [81.07, 236.49, 513.9, 505.73]\n",
      "Detected mug with confidence 0.765 at location [123.87, 123.23, 477.47, 487.02]\n",
      "Detected mug with confidence 0.775 at location [115.11, 111.82, 479.46, 471.46]\n",
      "Detected mug with confidence 0.78 at location [97.64, 110.52, 490.62, 474.16]\n",
      "Detected mug with confidence 0.767 at location [124.59, 114.36, 522.4, 497.79]\n",
      "Detected mug with confidence 0.828 at location [116.4, 112.9, 468.18, 382.62]\n",
      "Detected mug with confidence 0.802 at location [114.5, 110.8, 471.86, 442.7]\n",
      "true label: ball\n",
      "Detected mug with confidence 0.625 at location [61.81, 11.02, 524.1, 157.36]\n",
      "Detected mug with confidence 0.684 at location [25.81, 23.49, 413.02, 549.18]\n",
      "Detected mug with confidence 0.574 at location [38.95, 89.09, 217.52, 468.96]\n",
      "Detected mug with confidence 0.608 at location [225.0, 249.31, 431.47, 438.27]\n",
      "Detected mug with confidence 0.653 at location [51.09, 34.2, 531.57, 473.38]\n",
      "Detected mug with confidence 0.581 at location [-1.55, 23.38, 598.43, 337.57]\n",
      "Detected mug with confidence 0.527 at location [212.45, 89.3, 557.62, 530.06]\n",
      "Detected mug with confidence 0.58 at location [133.91, 33.75, 490.53, 266.57]\n",
      "Detected mug with confidence 0.621 at location [60.29, 34.29, 461.04, 465.89]\n",
      "Detected mug with confidence 0.635 at location [198.18, 87.08, 466.4, 441.97]\n",
      "true label: mug\n",
      "Detected mug with confidence 0.602 at location [18.31, 5.96, 448.51, 124.15]\n",
      "Detected mug with confidence 0.617 at location [13.13, 21.63, 316.63, 473.12]\n",
      "Detected mug with confidence 0.593 at location [38.62, 132.49, 264.49, 362.27]\n",
      "Detected mug with confidence 0.581 at location [72.62, 256.8, 269.27, 465.09]\n",
      "Detected mug with confidence 0.748 at location [134.07, 299.19, 324.72, 459.15]\n",
      "Detected mug with confidence 0.557 at location [233.96, 158.44, 571.35, 528.04]\n",
      "Detected mug with confidence 0.562 at location [186.08, 170.17, 365.03, 317.45]\n",
      "Detected mug with confidence 0.504 at location [11.17, 3.09, 599.67, 572.97]\n",
      "Detected mug with confidence 0.542 at location [59.16, 166.13, 212.28, 457.0]\n",
      "Detected mug with confidence 0.588 at location [94.28, 256.33, 333.93, 464.55]\n",
      "true label: mug\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported number of image dimensions: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(fn)\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrue label: \u001b[39m\u001b[39m{\u001b[39;00mtrue_label\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m inputs \u001b[39m=\u001b[39m image_processor(images\u001b[39m=\u001b[39;49mimage, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     33\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m     35\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/transformers/image_processing_utils.py:458\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    457\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/transformers/models/yolos/image_processing_yolos.py:1122\u001b[0m, in \u001b[0;36mYolosImageProcessor.preprocess\u001b[0;34m(self, images, annotations, return_segmentation_masks, masks_path, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_pad, format, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[39mdel\u001b[39;00m resized_images, resized_annotations\n\u001b[1;32m   1121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1122\u001b[0m         images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize(image, size\u001b[39m=\u001b[39msize, resample\u001b[39m=\u001b[39mresample) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m   1124\u001b[0m \u001b[39mif\u001b[39;00m do_rescale:\n\u001b[1;32m   1125\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image, rescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/transformers/models/yolos/image_processing_yolos.py:1122\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[39mdel\u001b[39;00m resized_images, resized_annotations\n\u001b[1;32m   1121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1122\u001b[0m         images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize(image, size\u001b[39m=\u001b[39;49msize, resample\u001b[39m=\u001b[39;49mresample) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m   1124\u001b[0m \u001b[39mif\u001b[39;00m do_rescale:\n\u001b[1;32m   1125\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image, rescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/transformers/models/yolos/image_processing_yolos.py:836\u001b[0m, in \u001b[0;36mYolosImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m size \u001b[39m=\u001b[39m get_size_dict(size, max_size\u001b[39m=\u001b[39mmax_size, default_to_square\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mshortest_edge\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m size \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlongest_edge\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m size:\n\u001b[0;32m--> 836\u001b[0m     size \u001b[39m=\u001b[39m get_resize_output_image_size(image, size[\u001b[39m\"\u001b[39;49m\u001b[39mshortest_edge\u001b[39;49m\u001b[39m\"\u001b[39;49m], size[\u001b[39m\"\u001b[39;49m\u001b[39mlongest_edge\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    837\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m size \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m size:\n\u001b[1;32m    838\u001b[0m     size \u001b[39m=\u001b[39m (size[\u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m], size[\u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/transformers/models/yolos/image_processing_yolos.py:156\u001b[0m, in \u001b[0;36mget_resize_output_image_size\u001b[0;34m(input_image, size, max_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_resize_output_image_size\u001b[39m(\n\u001b[1;32m    141\u001b[0m     input_image: np\u001b[39m.\u001b[39mndarray, size: Union[\u001b[39mint\u001b[39m, Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m], List[\u001b[39mint\u001b[39m]], max_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[1;32m    143\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m    Computes the output image size given the input image size and the desired output size. If the desired output size\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m    is a tuple or list, the output image size is returned as is. If the desired output size is an integer, the output\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m            The maximum allowed output size.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     image_size \u001b[39m=\u001b[39m get_image_size(input_image)\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    158\u001b[0m         \u001b[39mreturn\u001b[39;00m size\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/transformers/image_utils.py:201\u001b[0m, in \u001b[0;36mget_image_size\u001b[0;34m(image, channel_dim)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39mReturns the (height, width) dimensions of the image.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m    A tuple of the image's height and width.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m channel_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     channel_dim \u001b[39m=\u001b[39m infer_channel_dimension_format(image)\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m channel_dim \u001b[39m==\u001b[39m ChannelDimension\u001b[39m.\u001b[39mFIRST:\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.9/site-packages/transformers/image_utils.py:159\u001b[0m, in \u001b[0;36minfer_channel_dimension_format\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    157\u001b[0m     first_dim, last_dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported number of image dimensions: \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m image\u001b[39m.\u001b[39mshape[first_dim] \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m ChannelDimension\u001b[39m.\u001b[39mFIRST\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported number of image dimensions: 2"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(M, 3))\n",
    "\n",
    "def draw_boxes(box, label, image):\n",
    "    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "    color = COLORS[0]\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        (int(box[0]), int(box[1])),\n",
    "        (int(box[2]), int(box[3])),\n",
    "        color, 2\n",
    "    )\n",
    "    cv2.putText(image, label, (int(box[0]), int(box[1]-5)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n",
    "                lineType=cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "labels_test = {\n",
    "    0: 'ball',\n",
    "    1: 'mug',\n",
    "    2: 'pen'\n",
    "}\n",
    "\n",
    "for true_label in ('ball', 'mug', 'pen'):\n",
    "    for fn in glob(f'i2l-dataset/{true_label}/*.jpg'):\n",
    "        image = Image.open(fn)\n",
    "        print(f'true label: {true_label}')\n",
    "\n",
    "        inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        bboxes = outputs.pred_boxes\n",
    "\n",
    "        # print results\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        \n",
    "        results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "        for score, label, box in zip(results[\"scores\"][:10], results[\"labels\"], results[\"boxes\"]):\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            print(\n",
    "                f\"Detected {labels_test[label.item()]} with confidence \"\n",
    "                f\"{round(score.item(), 3)} at location {box}\"\n",
    "            )\n",
    "\n",
    "            image = draw_boxes(box, labels_test[label.item()], image)\n",
    "\n",
    "        #cv2.imshow('Image', image)\n",
    "        #cv2.waitKey(0)\n",
    "        cv2.imwrite(f'test-yolow-true_{fn.split(\"/\")[-1]}', image)\n",
    "        #input('next?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time: 0.566413164138794\n",
      "[ep 0] loss: 1.248409390449524\n",
      "inference time: 0.7442398071289062\n",
      "[ep 0] loss: 0.01679835096001625\n",
      "inference time: 0.3843560218811035\n",
      "[ep 0] loss: 7.556079387664795\n",
      "inference time: 0.35074400901794434\n",
      "[ep 0] loss: 8.062366485595703\n",
      "inference time: 0.5266430377960205\n",
      "[ep 0] loss: 5.394925117492676\n",
      "inference time: 0.3230311870574951\n",
      "[ep 0] loss: 2.286015033721924\n",
      "inference time: 0.3327350616455078\n",
      "[ep 0] loss: 4.939010143280029\n",
      "inference time: 0.32449793815612793\n",
      "[ep 0] loss: 0.832539439201355\n",
      "inference time: 0.3219747543334961\n",
      "[ep 0] loss: 1.7695698738098145\n",
      "inference time: 0.3338611125946045\n",
      "[ep 1] loss: 2.3216917514801025\n",
      "inference time: 0.5902340412139893\n",
      "[ep 1] loss: 0.23371371626853943\n",
      "inference time: 0.35471510887145996\n",
      "[ep 1] loss: 0.05608527734875679\n",
      "inference time: 0.3724658489227295\n",
      "[ep 1] loss: 0.04904178902506828\n",
      "inference time: 0.43518614768981934\n",
      "[ep 1] loss: 2.039592742919922\n",
      "inference time: 0.3842461109161377\n",
      "[ep 1] loss: 0.02461198903620243\n",
      "inference time: 0.36942100524902344\n",
      "[ep 1] loss: 1.19191575050354\n",
      "inference time: 0.5863680839538574\n",
      "[ep 1] loss: 5.404065132141113\n",
      "inference time: 0.7074117660522461\n",
      "[ep 1] loss: 5.319288730621338\n",
      "inference time: 0.37033605575561523\n",
      "[ep 2] loss: 0.07490614056587219\n",
      "inference time: 0.5806140899658203\n",
      "[ep 2] loss: 0.5184752941131592\n",
      "inference time: 0.38650989532470703\n",
      "[ep 2] loss: 0.0049429358914494514\n",
      "inference time: 0.37392187118530273\n",
      "[ep 2] loss: 1.7891249656677246\n",
      "inference time: 0.5848820209503174\n",
      "[ep 2] loss: 0.0006921279709786177\n",
      "inference time: 0.39020419120788574\n",
      "[ep 2] loss: 1.3269990682601929\n",
      "inference time: 0.37409305572509766\n",
      "[ep 2] loss: 0.9785364270210266\n",
      "inference time: 0.7643780708312988\n",
      "[ep 2] loss: 0.0744958221912384\n",
      "inference time: 0.3646872043609619\n",
      "[ep 2] loss: 1.0994597673416138\n",
      "inference time: 0.38401198387145996\n",
      "[ep 3] loss: 0.5830571055412292\n",
      "inference time: 0.4339439868927002\n",
      "[ep 3] loss: 0.012924239039421082\n",
      "inference time: 0.7491838932037354\n",
      "[ep 3] loss: 0.0013722298899665475\n",
      "inference time: 0.37631893157958984\n",
      "[ep 3] loss: 1.436787486076355\n",
      "inference time: 0.5981640815734863\n",
      "[ep 3] loss: 0.2929020822048187\n",
      "inference time: 0.3703751564025879\n",
      "[ep 3] loss: 0.09897436946630478\n",
      "inference time: 0.5971803665161133\n",
      "[ep 3] loss: 6.198863957251888e-06\n",
      "inference time: 0.36925506591796875\n",
      "[ep 3] loss: 1.5173165798187256\n",
      "inference time: 0.3746490478515625\n",
      "[ep 3] loss: 0.14680945873260498\n",
      "inference time: 0.7615978717803955\n",
      "[ep 4] loss: 0.004354993812739849\n",
      "inference time: 0.5702042579650879\n",
      "[ep 4] loss: 0.3325484097003937\n",
      "inference time: 0.3757169246673584\n",
      "[ep 4] loss: 0.01446969248354435\n",
      "inference time: 0.45188403129577637\n",
      "[ep 4] loss: 0.8183416724205017\n",
      "inference time: 0.3812899589538574\n",
      "[ep 4] loss: 0.10347329825162888\n",
      "inference time: 0.38884520530700684\n",
      "[ep 4] loss: 0.399901419878006\n",
      "inference time: 0.375399112701416\n",
      "[ep 4] loss: 0.13104696571826935\n",
      "inference time: 0.41367387771606445\n",
      "[ep 4] loss: 0.004854677710682154\n",
      "inference time: 0.6207692623138428\n",
      "[ep 4] loss: 6.9141146923357155e-06\n",
      "inference time: 0.374619722366333\n",
      "[ep 5] loss: 0.028898507356643677\n",
      "inference time: 0.3732428550720215\n",
      "[ep 5] loss: 0.07924243807792664\n",
      "inference time: 0.37680506706237793\n",
      "[ep 5] loss: 0.008356249891221523\n",
      "inference time: 0.37578797340393066\n",
      "[ep 5] loss: 0.18132182955741882\n",
      "inference time: 0.6173498630523682\n",
      "[ep 5] loss: 3.6954811548639555e-06\n",
      "inference time: 0.7735097408294678\n",
      "[ep 5] loss: 0.0011725700460374355\n",
      "inference time: 0.6515183448791504\n",
      "[ep 5] loss: 0.0013065143721178174\n",
      "inference time: 0.45942187309265137\n",
      "[ep 5] loss: 0.0005502378917299211\n",
      "inference time: 0.40115785598754883\n",
      "[ep 5] loss: 0.04171271249651909\n",
      "inference time: 0.6165211200714111\n",
      "[ep 6] loss: 4.410734163684538e-06\n",
      "inference time: 0.36295199394226074\n",
      "[ep 6] loss: 0.0006802152493037283\n",
      "inference time: 0.40662312507629395\n",
      "[ep 6] loss: 0.053916241973638535\n",
      "inference time: 0.37866902351379395\n",
      "[ep 6] loss: 0.12367503345012665\n",
      "inference time: 0.61857008934021\n",
      "[ep 6] loss: 0.0003780603874474764\n",
      "inference time: 0.3945002555847168\n",
      "[ep 6] loss: 0.03391338139772415\n",
      "inference time: 0.38642406463623047\n",
      "[ep 6] loss: 0.11708343029022217\n",
      "inference time: 0.45841479301452637\n",
      "[ep 6] loss: 0.00016830935783218592\n",
      "inference time: 0.7906608581542969\n",
      "[ep 6] loss: 0.0023813480511307716\n",
      "inference time: 0.40168094635009766\n",
      "[ep 7] loss: 0.06406952440738678\n",
      "inference time: 0.39284610748291016\n",
      "[ep 7] loss: 0.1278078705072403\n",
      "inference time: 0.6240110397338867\n",
      "[ep 7] loss: 0.00024673278676345944\n",
      "inference time: 0.38396215438842773\n",
      "[ep 7] loss: 0.0201546810567379\n",
      "inference time: 0.7709629535675049\n",
      "[ep 7] loss: 0.0025137036573141813\n",
      "inference time: 0.3892080783843994\n",
      "[ep 7] loss: 0.0340769998729229\n",
      "inference time: 0.44820594787597656\n",
      "[ep 7] loss: 0.000188332938705571\n",
      "inference time: 0.624021053314209\n",
      "[ep 7] loss: 1.7404405298293568e-05\n",
      "inference time: 0.3678159713745117\n",
      "[ep 7] loss: 0.0005432083853520453\n",
      "inference time: 0.37227392196655273\n",
      "[ep 8] loss: 0.00056429672986269\n",
      "inference time: 0.4378197193145752\n",
      "[ep 8] loss: 0.024279283359646797\n",
      "inference time: 0.411236047744751\n",
      "[ep 8] loss: 0.031046394258737564\n",
      "inference time: 0.413421630859375\n",
      "[ep 8] loss: 0.012365816161036491\n",
      "inference time: 0.7352230548858643\n",
      "[ep 8] loss: 2.52720492426306e-05\n",
      "inference time: 0.556020975112915\n",
      "[ep 8] loss: 0.00029416524921543896\n",
      "inference time: 0.3722250461578369\n",
      "[ep 8] loss: 0.12135399132966995\n",
      "inference time: 0.4504079818725586\n",
      "[ep 8] loss: 0.0003150205302517861\n",
      "inference time: 0.7737982273101807\n",
      "[ep 8] loss: 0.0028093892615288496\n",
      "inference time: 0.7619259357452393\n",
      "[ep 9] loss: 0.002686108462512493\n",
      "inference time: 0.38641977310180664\n",
      "[ep 9] loss: 0.010480484925210476\n",
      "inference time: 0.6049849987030029\n",
      "[ep 9] loss: 0.00038068200228735805\n",
      "inference time: 0.425037145614624\n",
      "[ep 9] loss: 0.00036507140612229705\n",
      "inference time: 0.3766748905181885\n",
      "[ep 9] loss: 0.022253794595599174\n",
      "inference time: 0.6314270496368408\n",
      "[ep 9] loss: 2.658331868587993e-05\n",
      "inference time: 0.37866783142089844\n",
      "[ep 9] loss: 0.001033010776154697\n",
      "inference time: 0.40926170349121094\n",
      "[ep 9] loss: 0.011327007785439491\n",
      "inference time: 0.38810300827026367\n",
      "[ep 9] loss: 0.08957967907190323\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from time import time\n",
    "from random import shuffle\n",
    "\n",
    "X = []\n",
    "Y_true = []\n",
    "\n",
    "for ob in ('ball', 'mug', 'pen'):\n",
    "    for i in (1, 3, 5):\n",
    "        image = Image.open(f'i2l-dataset/{ob}/{ob}_0{i}.jpg')\n",
    "        x = image_processor(images=image, return_tensors=\"pt\")['pixel_values']\n",
    "        X.append(x)\n",
    "        y = torch.tensor([1., 0., 0.]) if ob == 'ball' else torch.tensor([0., 1., 0.]) if ob == 'mug' else torch.tensor([0., 0., 1.])\n",
    "        Y_true.append(y)\n",
    "\n",
    "optimizer = AdamW(last_layer.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "inds = list(range(len(X)))\n",
    "\n",
    "for ep in range(10):\n",
    "    shuffle(inds)\n",
    "    for ind in inds:\n",
    "        x = X[ind]\n",
    "        y_true = Y_true[ind]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t0 = time()\n",
    "        outputs = model(**{'pixel_values': x})\n",
    "        print(f'inference time: {time()-t0}')\n",
    "\n",
    "        y_pred = outputs.logits[0][0]\n",
    "\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'[ep {ep}] loss: {loss.data}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
